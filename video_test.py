# import cv2
# import cv2.aruco as aruco
# import numpy as np
# import math
# import time
# import socket
# import json
# import threading
# import queue
# import os
#
# # ================= 1. é…ç½®åŒºåŸŸ =================
#
# # --- RTSP è¿æ¥é…ç½® ---
# RTSP_USERNAME = "djiuser"
# RTSP_PASSWORD = "123456"
# RTSP_IP = "192.168.31.214"  # é¥æ§å™¨ IP (M3E/M30 é€šå¸¸æ˜¯è¿™ä¸ª)
# RTSP_PORT = 8554
# RTSP_PATH = "streaming/live/1"
#
# # --- å®‰å“ç«¯é€šä¿¡é…ç½® ---
# ANDROID_IP = "172.27.46.80"
# ANDROID_PORT = 8888
#
# # --- ç‰©ç†å°ºå¯¸ä¸å¸ƒå±€ ---
# TAG_SIZE_BIG = 0.515
# TAG_SIZE_SMALL = 0.096
#
# TAG_LAYOUT = {
#     0: np.array([0.0, 0.0, 0.0]),
#     576: np.array([0.15, -0.15, 0.0]),
#     571: np.array([-0.15, 0.15, 0.0])
# }
#
# # --- åˆ†è¾¨ç‡è®¾ç½® ---
# # è¾“å…¥æµåˆ†è¾¨ç‡ (DJI M3E é€šå¸¸æ˜¯ 4K æˆ– 1080P)
# # æˆ‘ä»¬ä¸éœ€è¦æ‰‹åŠ¨æŒ‡å®šè¾“å…¥åˆ†è¾¨ç‡ï¼ŒOpenCV ä¼šè‡ªåŠ¨è¯†åˆ«
# # PROCESS_W/H æ˜¯ç®—æ³•å¤„ç†æ—¶çš„åˆ†è¾¨ç‡ï¼Œå»ºè®® 1920x1080 ä»¥å¹³è¡¡é€Ÿåº¦ä¸ç²¾åº¦
# PROCESS_W = 1920
# PROCESS_H = 1080
# DFOV_DEG = 84.0
#
#
# # ================= 2. çº¿ç¨‹å·¥å…·ç±» =================
#
# class RTSPStreamLoader:
#     """
#     RTSP ä¸“ç”¨æ‹‰æµçº¿ç¨‹ï¼š
#     1. è‡ªåŠ¨æ–­çº¿é‡è¿
#     2. æ°¸è¿œåªä¿ç•™æœ€æ–°ä¸€å¸§ (ä¸¢å¼ƒæ—§å¸§)
#     """
#
#     def __init__(self, rtsp_url):
#         self.rtsp_url = rtsp_url
#         self.lock = threading.Lock()
#         self.frame = None
#         self.stopped = False
#         self.connect_success = False
#
#         # å¼ºåˆ¶ä½¿ç”¨ UDP ä¼ è¾“ä»¥é™ä½å»¶è¿Ÿ
#         os.environ["OPENCV_FFMPEG_CAPTURE_OPTIONS"] = "rtsp_transport;udp"
#
#         print(f"ğŸ“¡ æ­£åœ¨è¿æ¥ RTSP: {rtsp_url} ...")
#         self.connect()
#
#     def connect(self):
#         if hasattr(self, 'cap') and self.cap:
#             self.cap.release()
#
#         self.cap = cv2.VideoCapture(self.rtsp_url)
#         # ç¼“å†²åŒºè®¾ç½®ä¸º1ï¼Œå°½å¯èƒ½å‡å°‘ç§¯å‹
#         self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
#
#         if self.cap.isOpened():
#             self.connect_success = True
#             print("âœ… RTSP è¿æ¥æˆåŠŸï¼")
#         else:
#             print("âŒ RTSP è¿æ¥å¤±è´¥ï¼Œå°†åœ¨åå°é‡è¯•...")
#
#     def start(self):
#         threading.Thread(target=self.update, args=(), daemon=True).start()
#         return self
#
#     def update(self):
#         while not self.stopped:
#             if not self.cap.isOpened():
#                 time.sleep(1)
#                 self.connect()
#                 continue
#
#             ret, frame = self.cap.read()
#             if ret:
#                 with self.lock:
#                     self.frame = frame
#             else:
#                 # è¯»å–å¤±è´¥ï¼ˆå¯èƒ½æ˜¯æ–­æµï¼‰
#                 print("âš ï¸ RTSP æ— æ•°æ®ï¼Œå°è¯•é‡è¿...")
#                 time.sleep(0.5)
#                 self.connect()
#
#     def get_frame(self):
#         with self.lock:
#             return self.frame.copy() if self.frame is not None else None
#
#     def stop(self):
#         self.stopped = True
#         if self.cap: self.cap.release()
#
#
# class UDPSender:
#     """
#     UDP å‘é€çº¿ç¨‹ï¼š
#     é¿å…ç½‘ç»œ IO é˜»å¡ä¸»çº¿ç¨‹çš„å›¾åƒå¤„ç†
#     """
#
#     def __init__(self, ip, port):
#         self.target_addr = (ip, port)
#         self.sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
#         self.queue = queue.Queue(maxsize=1)  # åªå­˜æœ€æ–°çš„ä¸€æ¡
#         self.stopped = False
#         print(f"ğŸ“¡ UDP å‘é€æœåŠ¡å¯åŠ¨ -> {ip}:{port}")
#
#     def start(self):
#         threading.Thread(target=self.run, args=(), daemon=True).start()
#         return self
#
#     def send_async(self, data):
#         if self.stopped: return
#         try:
#             # å¦‚æœé˜Ÿåˆ—æ»¡ï¼Œä¸¢å¼ƒæ—§çš„ï¼Œæ”¾å…¥æ–°çš„
#             if self.queue.full():
#                 try:
#                     self.queue.get_nowait()
#                 except:
#                     pass
#             self.queue.put_nowait(data)
#         except:
#             pass
#
#     def run(self):
#         while not self.stopped:
#             try:
#                 data = self.queue.get(timeout=0.5)
#                 msg = json.dumps(data).encode('utf-8')
#                 self.sock.sendto(msg, self.target_addr)
#             except queue.Empty:
#                 continue
#             except Exception as e:
#                 print(f"UDP Error: {e}")
#
#     def stop(self):
#         self.stopped = True
#         self.sock.close()
#
#
# # ================= 3. ç®—æ³•å·¥å…·ç±» =================
#
# def apply_deadband(val):
#     """æ­»åŒºæ§åˆ¶ï¼šé˜²æ­¢ç”±äºå™ªç‚¹å¯¼è‡´æ— äººæœºå¾®å°æŠ–åŠ¨"""
#     MIN_MOVE_SPEED = 0.12  # æœ€å°åŠ¨ä½œé€Ÿåº¦
#     STOP_THRESHOLD = 0.05  # æ­»åŒºé˜ˆå€¼
#
#     if abs(val) < STOP_THRESHOLD:
#         return 0.0
#     elif abs(val) < MIN_MOVE_SPEED:
#         return math.copysign(MIN_MOVE_SPEED, val)
#     else:
#         return val
#
#
# class PIDController:
#     def __init__(self, kp, ki, kd, max_out=0.5):
#         self.kp = kp
#         self.ki = ki
#         self.kd = kd
#         self.max_out = max_out
#         self.prev_error = 0
#         self.integral = 0
#
#     def update(self, error, dt):
#         if dt <= 0: dt = 0.033
#
#         p_term = self.kp * error
#         d_term = self.kd * (error - self.prev_error) / dt
#
#         self.integral += error * dt
#         self.integral = np.clip(self.integral, -0.5, 0.5)
#         i_term = self.ki * self.integral
#
#         output = p_term + i_term + d_term
#         self.prev_error = error
#         return np.clip(output, -self.max_out, self.max_out)
#
#
# class EnhancedTagDetector:
#     """
#     ã€ä¿ç•™å¢å¼ºæ£€æµ‹ã€‘
#     é›†æˆå£ç½©æ³•å’Œè†¨èƒ€æ³•ï¼Œä¿è¯åœ¨å¤æ‚ç¯å¢ƒä¸‹ä¹Ÿèƒ½è¯†åˆ«
#     """
#
#     def __init__(self, width, height, dfov_deg):
#         self.camera_matrix, self.dist_coeffs = self._get_camera_matrix(width, height, dfov_deg)
#         self.aruco_dict = aruco.getPredefinedDictionary(aruco.DICT_APRILTAG_36h11)
#         self.parameters = aruco.DetectorParameters()
#         self.parameters.cornerRefinementMethod = aruco.CORNER_REFINE_SUBPIX
#         self.detector = aruco.ArucoDetector(self.aruco_dict, self.parameters)
#
#     def _get_camera_matrix(self, width, height, dfov_deg):
#         diagonal_pixels = math.sqrt(width ** 2 + height ** 2)
#         fov_rad = math.radians(dfov_deg)
#         f_px = (diagonal_pixels / 2) / math.tan(fov_rad / 2)
#         K = np.array([[f_px, 0, width / 2], [0, f_px, height / 2], [0, 0, 1]], dtype=np.float32)
#         D = np.zeros((5, 1))
#         return K, D
#
#     def detect_with_enhancement(self, frame):
#         gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
#
#         # 1. å¸¸è§„æ£€æµ‹
#         corners, ids, _ = self.detector.detectMarkers(gray)
#         final_corners = list(corners) if corners else []
#         final_ids = list(ids[:, 0]) if ids is not None else []
#
#         has_big_tag = 0 in final_ids if final_ids else False
#
#         # 2. å£ç½©æ³•
#         if not has_big_tag:
#             corners_mask, ids_mask = self._mask_small_tags_method(gray, final_corners, final_ids)
#             if ids_mask is not None and 0 in ids_mask:
#                 final_corners.extend(corners_mask)
#                 final_ids.extend(ids_mask[:, 0])
#                 has_big_tag = True
#
#         # 3. è†¨èƒ€æ³•
#         if not has_big_tag:
#             corners_dilate, ids_dilate = self._dilate_method(gray)
#             if ids_dilate is not None and 0 in ids_dilate:
#                 final_corners.extend(corners_dilate)
#                 final_ids.extend(ids_dilate[:, 0])
#
#         if not final_ids: return tuple(), None
#         return tuple(final_corners), np.array(final_ids).reshape(-1, 1)
#
#     def _mask_small_tags_method(self, gray, existing_corners, existing_ids):
#         masked_gray = gray.copy()
#         small_tags_corners = []
#         if existing_ids:
#             for i, id_val in enumerate(existing_ids):
#                 if id_val in [571, 576]:
#                     small_tags_corners.append(existing_corners[i])
#         for corner_set in small_tags_corners:
#             pts = corner_set[0]
#             center = np.mean(pts, axis=0)
#             expanded_pts = (pts - center) * 1.6 + center
#             cv2.fillPoly(masked_gray, [expanded_pts.astype(np.int32)], 255)
#         corners, ids, _ = self.detector.detectMarkers(masked_gray)
#         if ids is not None and 0 in ids:
#             return [corners[np.where(ids == 0)[0][0]]], np.array([[0]])
#         return None, None
#
#     def _dilate_method(self, gray):
#         k_size = int(gray.shape[1] * 0.015)
#         if k_size % 2 == 0: k_size += 1
#         dilated = cv2.dilate(gray, np.ones((k_size, k_size), np.uint8), iterations=1)
#         corners, ids, _ = self.detector.detectMarkers(dilated)
#         if ids is not None and 0 in ids:
#             return [corners[np.where(ids == 0)[0][0]]], np.array([[0]])
#         return None, None
#
#     def solve_position(self, corners, ids):
#         if ids is None: return None
#         positions = []
#         for i, id_val in enumerate(ids[:, 0]):
#             curr_id = int(id_val)
#             size = TAG_SIZE_SMALL if curr_id in [571, 576] else TAG_SIZE_BIG
#             obj_pts = np.array([[-size / 2, size / 2, 0], [size / 2, size / 2, 0],
#                                 [size / 2, -size / 2, 0], [-size / 2, -size / 2, 0]], dtype=np.float32)
#
#             # ä½¿ç”¨åŸºç¡€ solvePnP (ä¸ä½¿ç”¨ IPPE ä»¥é¿å…æŠ–åŠ¨ï¼Œæˆ–æŒ‰éœ€å¼€å¯)
#             _, rvec, tvec = cv2.solvePnP(obj_pts, corners[i], self.camera_matrix, self.dist_coeffs)
#
#             if curr_id in TAG_LAYOUT:
#                 positions.append(tvec.flatten() - TAG_LAYOUT[curr_id])
#
#         if positions: return np.mean(positions, axis=0)
#         return None
#
#
# # ================= 4. ä¸»æµç¨‹ =================
# def main():
#     # 1. å¯åŠ¨ç¡¬ä»¶çº¿ç¨‹
#     rtsp_url = f"rtsp://{RTSP_USERNAME}:{RTSP_PASSWORD}@{RTSP_IP}:{RTSP_PORT}/{RTSP_PATH}"
#     stream_loader = RTSPStreamLoader(rtsp_url).start()
#     udp_sender = UDPSender(ANDROID_IP, ANDROID_PORT).start()
#
#     # 2. åˆå§‹åŒ–å¢å¼ºå‹æ£€æµ‹å™¨
#     detector = EnhancedTagDetector(PROCESS_W, PROCESS_H, DFOV_DEG)
#
#     # 3. åˆå§‹åŒ– PID
#     pid_roll = PIDController(kp=1.0, ki=0.0, kd=0.1)
#     pid_pitch = PIDController(kp=1.0, ki=0.0, kd=0.1)
#
#     print("ğŸš€ èåˆç³»ç»Ÿå…¨é€Ÿè¿è¡Œä¸­... (ç­‰å¾…è§†é¢‘æµ)")
#
#     # ç­‰å¾…ç¬¬ä¸€å¸§
#     while stream_loader.get_frame() is None:
#         time.sleep(0.1)
#
#     last_time = time.time()
#
#     try:
#         while True:
#             # --- A. è·å–æœ€æ–°å¸§ ---
#             frame = stream_loader.get_frame()
#             if frame is None: continue
#
#             # è®¡ç®— dt
#             current_time = time.time()
#             dt = current_time - last_time
#             last_time = current_time
#
#             # ç¼©æ”¾è‡³ 1080P å¤„ç† (æå‡é€Ÿåº¦)
#             if frame.shape[1] != PROCESS_W:
#                 frame = cv2.resize(frame, (PROCESS_W, PROCESS_H))
#
#             # --- B. å¢å¼ºè§†è§‰å¤„ç† ---
#             corners, ids = detector.detect_with_enhancement(frame)
#             final_pos = detector.solve_position(corners, ids)
#
#             # --- C. PID è®¡ç®—ä¸æ­»åŒºæ§åˆ¶ ---
#             cmd_roll, cmd_pitch, vel_z = 0.0, 0.0, 0.0
#
#             if final_pos is not None:
#                 x, y, z = final_pos
#
#                 # åŸå§‹ PID è¾“å‡º
#                 raw_roll = pid_roll.update(0 - x, dt=dt) * -1
#                 raw_pitch = pid_pitch.update(0 - y, dt=dt)
#
#                 # åº”ç”¨æ­»åŒº (é˜²æ­¢å¾®å°æŠ–åŠ¨)
#                 cmd_roll = apply_deadband(raw_roll)
#                 cmd_pitch = apply_deadband(raw_pitch)
#
#                 # ä¸‹é™é€»è¾‘
#                 horizontal_error = math.sqrt(x ** 2 + y ** 2)
#                 vel_z = -0.3 if horizontal_error < 0.2 else 0.0
#
#                 # --- ç»˜åˆ¶ UI (ä¿ç•™ä½ å–œæ¬¢çš„å¤§å­—ä½“é£æ ¼) ---
#                 aruco.drawDetectedMarkers(frame, corners, ids)
#
#                 # ä¸­å¿ƒç‚¹ä¸è¿çº¿
#                 cx = int(PROCESS_W / 2 + x * (detector.camera_matrix[0][0] / z))
#                 cy = int(PROCESS_H / 2 + y * (detector.camera_matrix[1][1] / z))
#                 screen_cx, screen_cy = PROCESS_W // 2, PROCESS_H // 2
#
#                 cv2.line(frame, (screen_cx, screen_cy), (cx, cy), (0, 255, 0), 3)
#                 cv2.circle(frame, (cx, cy), 15, (0, 0, 255), -1)
#
#                 # æ–‡å­—ä¿¡æ¯
#                 info_text = [
#                     f"H: {z:.2f}m",
#                     f"Err: {horizontal_error:.2f}m",
#                     f"R: {cmd_roll:.2f} P: {cmd_pitch:.2f}"
#                 ]
#                 for idx, text in enumerate(info_text):
#                     cv2.putText(frame, text, (30, 80 + idx * 60),
#                                 cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 255), 3)
#
#                 # ç®­å¤´æŒ‡ç¤º
#                 arrow_len = 150
#                 end_x = int(screen_cx + cmd_roll * arrow_len)
#                 end_y = int(screen_cy - cmd_pitch * arrow_len)
#                 cv2.arrowedLine(frame, (screen_cx, screen_cy), (end_x, end_y), (255, 0, 255), 8)
#             else:
#                 cv2.putText(frame, "SEARCHING...", (50, 150),
#                             cv2.FONT_HERSHEY_SIMPLEX, 2.5, (0, 0, 255), 5)
#
#             # --- D. å¼‚æ­¥å‘é€ ---
#             send_data = {
#                 "r": float(cmd_roll),
#                 "p": float(cmd_pitch),
#                 "y": 0.0,
#                 "t": float(vel_z)
#             }
#             udp_sender.send_async(send_data)
#
#             # --- E. æ˜¾ç¤º ---
#             # ç¼©å°ä¸€ç‚¹æ˜¾ç¤ºï¼Œé˜²æ­¢å æ»¡å±å¹•
#             show_frame = cv2.resize(frame, (960, 540))
#             # æ˜¾ç¤º FPS
#             fps = 1.0 / max(dt, 0.001)
#             cv2.putText(show_frame, f"FPS: {fps:.1f}", (20, 40),
#                         cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
#
#             cv2.imshow("M3E Fusion Control", show_frame)
#
#             if cv2.waitKey(1) & 0xFF == ord('q'):
#                 break
#
#     except KeyboardInterrupt:
#         pass
#     finally:
#         print("ğŸ›‘ æ­£åœ¨åœæ­¢çº¿ç¨‹...")
#         stream_loader.stop()
#         udp_sender.stop()
#         cv2.destroyAllWindows()
#
#
# if __name__ == "__main__":
#     main()


import cv2
import cv2.aruco as aruco
import numpy as np
import math
import time

# ================= 1. ç”¨æˆ·é…ç½®åŒºåŸŸ =================
VIDEO_PATH = "test2.mp4"  # ä½ çš„è§†é¢‘æ–‡ä»¶è·¯å¾„
# å¦‚æœè®¾ä¸º 0ï¼Œåˆ™ä¼šè°ƒç”¨ç”µè„‘çš„æ‘„åƒå¤´è¿›è¡Œå®æ—¶æµ‹è¯•
# VIDEO_PATH = 0

# å¿…é¡»ä¸ä½ å½•åˆ¶è§†é¢‘æ—¶çš„åˆ†è¾¨ç‡/ç›¸æœºå‚æ•°ä¸€è‡´
STREAM_W = 3840
STREAM_H = 2160
DFOV_DEG = 84.0

# ç‰©ç†å°ºå¯¸ (ç±³)
TAG_SIZE_BIG = 0.515
TAG_SIZE_SMALL = 0.096

# å¸ƒå±€åç§»é‡ (è¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹)
TAG_LAYOUT = {
    0: np.array([0.0, 0.0, 0.0]),
    576: np.array([0.15, -0.15, 0.0]),
    571: np.array([-0.15, 0.15, 0.0])
}


# ================= 2. å·¥å…·ç±» =================

class PIDController:
    def __init__(self, kp, ki, kd, max_out=1.0):
        self.kp = kp
        self.ki = ki
        self.kd = kd
        self.max_out = max_out
        self.prev_error = 0
        self.integral = 0

    def update(self, error, dt):
        # è§†é¢‘å›æ”¾æ—¶ï¼Œå¦‚æœä¸é™åˆ¶ Iï¼Œç§¯åˆ†é¡¹ä¼šå› ä¸ºâ€œè¯¯å·®ä¸€ç›´æ²¡æ¶ˆé™¤â€è€Œæ— é™å¢å¤§
        # æ‰€ä»¥æµ‹è¯•è§†é¢‘æ—¶å»ºè®®æŠŠ I å…³æ‰ï¼Œæˆ–è€…ç»™ä¸€ä¸ªå¾ˆå¼ºçš„æŠ—é¥±å’Œé™åˆ¶
        if dt <= 0: dt = 0.033

        p_term = self.kp * error
        d_term = self.kd * (error - self.prev_error) / dt

        # ç®€å•çš„ç§¯åˆ†é€»è¾‘
        self.integral += error * dt
        self.integral = np.clip(self.integral, -0.5, 0.5)  # å¼ºåŠ›é™åˆ¶
        i_term = self.ki * self.integral

        output = p_term + i_term + d_term
        self.prev_error = error
        return np.clip(output, -self.max_out, self.max_out)


def get_camera_matrix(width, height, dfov_deg):
    diagonal_pixels = math.sqrt(width ** 2 + height ** 2)
    fov_rad = math.radians(dfov_deg)
    f_px = (diagonal_pixels / 2) / math.tan(fov_rad / 2)
    K = np.array([[f_px, 0, width / 2], [0, f_px, height / 2], [0, 0, 1]], dtype=np.float32)
    D = np.zeros((5, 1))
    return K, D


# ================= 3. ä¸»ç¨‹åº =================

def main():
    # åˆå§‹åŒ–
    K, D = get_camera_matrix(STREAM_W, STREAM_H, DFOV_DEG)

    aruco_dict = aruco.getPredefinedDictionary(aruco.DICT_APRILTAG_36h11)
    params = aruco.DetectorParameters()
    detector = aruco.ArucoDetector(aruco_dict, params)

    # åˆå§‹åŒ– PID (æ³¨æ„ï¼šå›æ”¾è§†é¢‘æ—¶å»ºè®® Ki=0ï¼Œå› ä¸ºæ— äººæœºå®é™…ä¸Šä¸ä¼šåŠ¨ï¼Œè¯¯å·®ä¸ä¼šæ¶ˆé™¤)
    pid_roll = PIDController(kp=1.0, ki=0.0, kd=0.1)  # æ§åˆ¶å·¦å³
    pid_pitch = PIDController(kp=1.0, ki=0.0, kd=0.1)  # æ§åˆ¶å‰å

    # æ‰“å¼€è§†é¢‘
    cap = cv2.VideoCapture(VIDEO_PATH)
    if not cap.isOpened():
        print(f"âŒ æ— æ³•æ‰“å¼€è§†é¢‘: {VIDEO_PATH}")
        return

    print(f"ğŸ¬ å¼€å§‹æ’­æ”¾è§†é¢‘æµ‹è¯•... (æŒ‰ 'q' é€€å‡º, 'SPACE' æš‚åœ)")

    is_paused = False

    while True:
        if not is_paused:
            ret, frame = cap.read()
            if not ret:
                print("âœ… è§†é¢‘æ’­æ”¾ç»“æŸ")
                break

            # å¼ºåˆ¶ Resize (å¦‚æœè§†é¢‘åˆ†è¾¨ç‡ä¸å¯¹)
            if frame.shape[1] != STREAM_W:
                frame = cv2.resize(frame, (STREAM_W, STREAM_H))

            # --- æ ¸å¿ƒå¤„ç†é€»è¾‘ ---
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            corners, ids, _ = detector.detectMarkers(gray)

            detected_positions = []

            if ids is not None:
                for i, id_val in enumerate(ids[:, 0]):
                    # 1. å°ºå¯¸é€‰æ‹©
                    size = TAG_SIZE_SMALL if id_val in [571, 576] else TAG_SIZE_BIG

                    # 2. PnP
                    obj_pts = np.array([[-size / 2, size / 2, 0], [size / 2, size / 2, 0],
                                        [size / 2, -size / 2, 0], [-size / 2, -size / 2, 0]], dtype=np.float32)
                    _, rvec, tvec = cv2.solvePnP(obj_pts, corners[i], K, D)

                    # 3. ç©ºé—´æ˜ å°„ (è½¬æ¢åˆ°æ¿å­ä¸­å¿ƒ)
                    pos_cam = tvec.flatten()
                    if id_val in TAG_LAYOUT:
                        pos_center = pos_cam - TAG_LAYOUT[id_val]
                        detected_positions.append(pos_center)

                        # ç”»è½´
                        cv2.drawFrameAxes(frame, K, D, rvec, tvec, size)

            # --- å†³ç­–å¯è§†åŒ– ---
            # --- å†³ç­–å¯è§†åŒ– ---
            if detected_positions:
                # èåˆ
                final_pos = np.mean(detected_positions, axis=0)
                x, y, z = final_pos

                # è®¡ç®— PID
                cmd_roll = pid_roll.update(0 - x, dt=0.033) * -1  # å–å
                cmd_pitch = pid_pitch.update(0 - y, dt=0.033)  # ä¿æŒè´Ÿå·é€»è¾‘

                # --- ç»˜åˆ¶ UI ---
                # 1. ç”»å‡ºæ¨ç®—çš„æ¿å­ä¸­å¿ƒ (çº¢ç‚¹)
                cx = int(STREAM_W / 2 + x * (K[0][0] / z))
                cy = int(STREAM_H / 2 + y * (K[1][1] / z))

                # å±å¹•ä¸­å¿ƒ (æ— äººæœºå½“å‰ä½ç½®)
                screen_cx, screen_cy = STREAM_W // 2, STREAM_H // 2

                # è¿çº¿ (ç»¿è‰²è¯¯å·®çº¿) - ã€ä¿®æ”¹ã€‘ç¨å¾®åŠ ç²—äº†ä¸€ç‚¹
                cv2.line(frame, (screen_cx, screen_cy), (cx, cy), (0, 255, 0), 3)
                cv2.circle(frame, (cx, cy), 15, (0, 0, 255), -1)

                # 2. æ˜¾ç¤ºæ•°æ®é¢æ¿
                info_text = [
                    f"Height: {z:.2f}m",
                    f"Error X: {x:.2f}m",
                    f"Error Y: {y:.2f}m",
                    f"CMD Roll: {cmd_roll:.2f}",
                    f"CMD Pitch: {cmd_pitch:.2f}"
                ]

                # ã€æ ¸å¿ƒä¿®æ”¹åŒºåŸŸï¼šè°ƒå¤§å­—ä½“å’Œé—´è·ã€‘
                # fontScale: ä» 1 æ”¹ä¸º 1.8 (å˜å¤§)
                # thickness: ä» 2 æ”¹ä¸º 3 (å˜ç²—)
                # è¡Œé—´è·: Yåæ ‡åç§»é‡ä» 40 æ”¹ä¸º 70 (é˜²æ­¢é‡å )
                # èµ·å§‹Yåæ ‡: ä» 80 æ”¹ä¸º 100 (å¾€ä¸‹æŒªä¸€ç‚¹ç•™å‡ºè¾¹è·)
                for idx, text in enumerate(info_text):
                    cv2.putText(frame, text, (50, 100 + idx * 70),
                                cv2.FONT_HERSHEY_SIMPLEX, 1.8, (0, 255, 255), 3)

                # 3. åŠ¨æ€ç®­å¤´ (æ¨¡æ‹Ÿæ§åˆ¶æŒ‡ä»¤) - ã€ä¿®æ”¹ã€‘ç®­å¤´åŠ ç²—åˆ° 8
                arrow_len = 150  # ã€ä¿®æ”¹ã€‘ç®­å¤´é•¿åº¦ä¹Ÿç¨å¾®åŠ é•¿äº†ä¸€ç‚¹
                end_x = int(screen_cx + cmd_roll * arrow_len)
                end_y = int(screen_cy - cmd_pitch * arrow_len)
                cv2.arrowedLine(frame, (screen_cx, screen_cy), (end_x, end_y), (255, 0, 255), 8)

            else:
                # ã€æ ¸å¿ƒä¿®æ”¹åŒºåŸŸï¼šè°ƒå¤§ Searching æç¤ºã€‘
                # fontScale: ä» 1.5 æ”¹ä¸º 2.5
                # thickness: ä» 3 æ”¹ä¸º 5
                cv2.putText(frame, "SEARCHING...", (50, 150),
                            cv2.FONT_HERSHEY_SIMPLEX, 2.5, (0, 0, 255), 5)

            # æ˜¾ç¤ºç»“æœ
            show_frame = cv2.resize(frame, (960, 540))  # ç¼©å°ä¸€ç‚¹æ–¹ä¾¿çœ‹
            cv2.imshow("Video Playback Test", show_frame)

        # é”®ç›˜æ§åˆ¶
        key = cv2.waitKey(30) & 0xFF  # 30ms å»¶æ—¶ï¼Œçº¦ç­‰äº 30fps å›æ”¾
        if key == ord('q'):
            break
        elif key == ord(' '):  # ç©ºæ ¼é”®æš‚åœ
            is_paused = not is_paused

    cap.release()
    cv2.destroyAllWindows()


if __name__ == "__main__":
    main()